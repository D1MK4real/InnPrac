{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pydiffvg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import covergan\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import importlib, sys\n",
    "# importlib.reload(covergan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/home/dmitriy/OVE/official_data/artemis_dataset_release_v0.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import covergan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from colorer.music_palette_dataset import PaletteDataset,ImageDataset\n",
    "dataset = PaletteDataset(8,1000,'name','/home/dmitriy/OVE/covergan/weights','/home/dmitriy/OVE/wikiart',pandas_dir='/home/dmitriy/OVE/official_data/artemis_dataset_release_v0.csv')\n",
    "dataset_image = ImageDataset(8,100,'name','/home/dmitriy/OVE/covergan/weights','/home/dmitriy/OVE/wikiart',pandas_dir='/home/dmitriy/OVE/official_data/artemis_dataset_release_v0.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from colorer.models.colorer_dropout import Colorer2\n",
    "z_dim=32\n",
    "num_gen_layers=5\n",
    "colors_count = 8\n",
    "colorer = Colorer2(\n",
    "        z_dim=z_dim,\n",
    "        audio_embedding_dim=9,\n",
    "        num_layers=num_gen_layers,\n",
    "        colors_count=colors_count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "Colorer2(\n  (model_): Sequential(\n    (0): Linear(in_features=41, out_features=35, bias=True)\n    (1): Dropout(p=0.2, inplace=False)\n    (2): BatchNorm1d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): LeakyReLU(negative_slope=0.2)\n    (4): Linear(in_features=35, out_features=29, bias=True)\n    (5): Dropout(p=0.2, inplace=False)\n    (6): BatchNorm1d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2)\n    (8): Linear(in_features=29, out_features=23, bias=True)\n    (9): Dropout(p=0.2, inplace=False)\n    (10): BatchNorm1d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): LeakyReLU(negative_slope=0.2)\n    (12): Linear(in_features=23, out_features=17, bias=True)\n    (13): Dropout(p=0.2, inplace=False)\n    (14): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (15): LeakyReLU(negative_slope=0.2)\n    (16): Linear(in_features=17, out_features=24, bias=True)\n    (17): Sigmoid()\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colorer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from colorer.models.gan_colorer import ColorerDiscriminator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "disc = ColorerDiscriminator(audio_embedding_dim=9, num_layers=2,colors_count= colors_count).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "ColorerDiscriminator(\n  (adv_layer): Sequential(\n    (0): Linear(in_features=33, out_features=17, bias=True)\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=17, out_features=8, bias=True)\n    (4): Sigmoid()\n  )\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def weighted_mse_loss(input, target, weight=None):\n",
    "    if weight is None:\n",
    "        max_weight = input.size()[1]\n",
    "        weight = torch.tensor([(max_weight - i // 3) // 3 for i in range(max_weight)]).to(input.device)\n",
    "        weight = weight.repeat((len(input), 1))\n",
    "    return (weight * (input - target) ** 2).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_noise(n_samples, input_dim, device):\n",
    "    \"\"\"\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    \"\"\"\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size=32\n",
    "train_dataloader = DataLoader(dataset,drop_last=True, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_checkpoint_filename(checkpoint_root: str, checkpoint_name: str, epoch: int = None) -> str:\n",
    "    suffix = f\"-{epoch}\" if epoch is not None else \"\"\n",
    "    return f\"{checkpoint_root}/{checkpoint_name}{suffix}.pt\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from captioner_train import logger\n",
    "import os\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_root: str, checkpoint_name: str,\n",
    "                    models) -> int:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    filename = get_checkpoint_filename(checkpoint_root, checkpoint_name)\n",
    "\n",
    "    if os.path.isfile(filename):\n",
    "        logger.info(f\"Found {filename}, loading\")\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        for i, model in enumerate(models):\n",
    "            model.load_state_dict(checkpoint[f\"{i}_state_dict\"])\n",
    "            print('Loaded')\n",
    "        epochs_done = checkpoint[f\"epochs_done\"]\n",
    "        logger.info(f\"{filename} loaded\")\n",
    "        return epochs_done\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_root: str, checkpoint_name: str, epochs_done: int, backup_epochs: int,\n",
    "                    models):\n",
    "    checkpoint_dict = {}\n",
    "    for i, model in enumerate(models):\n",
    "        checkpoint_dict[f\"{i}_state_dict\"] = model.state_dict()\n",
    "    checkpoint_dict[f\"epochs_done\"] = epochs_done\n",
    "\n",
    "    if not backup_epochs:\n",
    "        # Unconditional save\n",
    "        filename = get_checkpoint_filename(checkpoint_root, checkpoint_name)\n",
    "        torch.save(checkpoint_dict, filename)\n",
    "        logger.info(f\"{filename} saved\")\n",
    "    if backup_epochs and epochs_done and epochs_done % backup_epochs == 0:\n",
    "        # Regular backup\n",
    "        filename = get_checkpoint_filename(checkpoint_root, checkpoint_name, epochs_done)\n",
    "        torch.save(checkpoint_dict, filename)\n",
    "        logger.info(f\"Backup {filename} saved\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "ColorerDiscriminator(\n  (adv_layer): Sequential(\n    (0): Linear(in_features=33, out_features=17, bias=True)\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=17, out_features=8, bias=True)\n    (4): Sigmoid()\n  )\n)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "z_dim=9\n",
    "num_gen_layers=5\n",
    "colors_count = 8\n",
    "colorer = Colorer2(\n",
    "        z_dim=z_dim,\n",
    "        audio_embedding_dim=9,\n",
    "        num_layers=num_gen_layers,\n",
    "        colors_count=colors_count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "n_epochs = 5# training_params[\"n_epochs\"]\n",
    "lr = 3e-4\n",
    "z_dim = 9\n",
    "checkpoint_root ='/home/dmitriy/OVE/covergan/weights'\n",
    "backup_epochs = 1\n",
    "disc_slices = 6\n",
    "colors_count = 8\n",
    "gen_opt = torch.optim.Adam(colorer.parameters(), lr=lr)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# model_name = f'colorer_{colorer.color_type}_{colorer.colors_count}_colors'\n",
    "model_name = f'colorer_{colorer.colors_count}_colors_{train_dataloader.dataset.sorted_color}'\n",
    "print(\"Trying to load checkpoint.\")\n",
    "epochs_done = load_checkpoint(checkpoint_root, model_name, [colorer, gen_opt])\n",
    "\n",
    "if epochs_done:\n",
    "    logger.info(f\"Loaded a checkpoint with {epochs_done} epochs done\")\n",
    "\n",
    "log_interval = 5\n",
    "# for epoch in range(epochs_done + 1, n_epochs + epochs_done + 1):\n",
    "#     colorer.train()\n",
    "#     running_D_test_loss = 0.0\n",
    "#     running_G_test_loss = 0.0\n",
    "#     count = 0\n",
    "#     for X_batch, y_batch in tqdm(train_dataloader):\n",
    "#         torch.cuda.empty_cache()\n",
    "#         y_batch = y_batch.to(device)\n",
    "#\n",
    "#         cur_batch_size = len(X_batch)\n",
    "#         X_batch = X_batch.float().to(device)\n",
    "#         X_batch_disc = X_batch\n",
    "#\n",
    "#         real_outputs = disc(X_batch_disc, y_batch)\n",
    "#         real_label = torch.ones(y_batch.shape[0], colors_count).to(device)\n",
    "#         z = get_noise(cur_batch_size, z_dim, device=device)\n",
    "#\n",
    "#         fake_inputs = colorer(z, X_batch_disc)\n",
    "#         fake_outputs = disc(X_batch_disc, fake_inputs)\n",
    "#\n",
    "#         fake_label = torch.zeros(fake_inputs.shape[0], colors_count).to(device)\n",
    "#         outputs = torch.cat((real_outputs, fake_outputs), dim=0)\n",
    "#         targets = torch.cat((real_label, fake_label), dim=0)\n",
    "#         D_loss = criterion(outputs, targets)\n",
    "#         running_D_test_loss += D_loss.item()\n",
    "#         disc_opt.zero_grad()\n",
    "#         D_loss.backward()\n",
    "#         disc_opt.step()\n",
    "#         gen_opt.zero_grad()\n",
    "#         z = get_noise(cur_batch_size, z_dim, device=device)\n",
    "#\n",
    "#         fake_inputs = colorer(z, X_batch_disc)\n",
    "#         fake_outputs = disc(X_batch_disc, fake_inputs)\n",
    "#         fake_targets = torch.ones(fake_inputs.shape[0], colors_count).to(device)\n",
    "#         G_loss = criterion(fake_outputs, fake_targets)\n",
    "#         gen_opt.zero_grad()\n",
    "#         G_loss.backward()\n",
    "#         gen_opt.step()\n",
    "#         running_G_test_loss += G_loss.item()\n",
    "#\n",
    "#         if (count + 1) % log_interval == 0:\n",
    "#                 print('Epoch {} Iteration {}: discriminator_loss {:.3f} generator_loss {:.3f}'\n",
    "#                       .format(epoch, count, D_loss.item(), G_loss.item()))\n",
    "#                 save_checkpoint(checkpoint_root, model_name, epoch, backup_epochs, [colorer, disc, gen_opt, disc_opt])\n",
    "#         count+=1\n",
    "#     avg_G_test_loss = running_G_test_loss / (count + 1)\n",
    "#     avg_D_test_loss = running_D_test_loss / (count+ 1)\n",
    "#     print('Train LOSS: colorer {}, disc {}'.format(avg_G_test_loss, avg_D_test_loss))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from outer.models.discriminator import Discriminator\n",
    "from outer.models.my_gen_fixed_6figs32_good import MyGeneratorFixedSixFigs32Good\n",
    "\n",
    "generator_type = MyGeneratorFixedSixFigs32Good\n",
    "discriminator_type = Discriminator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "num_gen_layers = 4\n",
    "num_disc_conv_layers = 3\n",
    "num_disc_linear_layers = 2\n",
    "z_dim = 32  # Dimension of the noise vector\n",
    "# z_dim = 512  # Dimension of the noise vector\n",
    "\n",
    "# Painter properties\n",
    "path_count = 3\n",
    "path_segment_count = 4\n",
    "disc_slices = 6\n",
    "max_stroke_width = 0.01  # relative to the canvas size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "gen = MyGeneratorFixedSixFigs32Good(9,9,5,400,max_stroke_width)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "disc = Discriminator(400,9,3,2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "400"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dataset_image[6][1].shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def plot_losses(epoch: int, cur_step: int, display_steps: int, bin_steps: int, losses: [(str, [float])]):\n",
    "    if cur_step % display_steps == 0 and cur_step > 0:\n",
    "        loss_stats = []\n",
    "\n",
    "        for loss_name, loss_values in losses:\n",
    "            loss_mean = sum(loss_values[-display_steps:]) / display_steps\n",
    "            if \"loss\" not in loss_name.lower() and \"metric\" not in loss_name.lower():\n",
    "                loss_name += \" Loss\"\n",
    "            loss_stats.append(f\"{loss_name}: {loss_mean}\")\n",
    "\n",
    "            num_examples = (len(loss_values) // bin_steps) * bin_steps\n",
    "            plt.plot(\n",
    "                range(num_examples // bin_steps),\n",
    "                torch.Tensor(loss_values[:num_examples]).view(-1, bin_steps).mean(1),\n",
    "                label=loss_name\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Epoch {epoch} (step {cur_step}): \" + \", \".join(loss_stats))\n",
    "        plt.legend()\n",
    "        print(\"Saving losses to png file\")\n",
    "        import covergan_train\n",
    "        plt.savefig(f\"{covergan_train.logger.plots_dir}/losses-{epoch}-{cur_step}.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    elif cur_step == 0:\n",
    "        logger.info(\"The training is working\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def get_gradient_penalty(disc, real, fake, audio_embedding):\n",
    "    # Mix the images together\n",
    "    epsilon = torch.rand((real.size(0), 1, 1, 1), device=real.device)\n",
    "    mixed_images = (real * epsilon + fake * (1 - epsilon)).requires_grad_(True)\n",
    "\n",
    "    # Calculate the critic's scores on the mixed images\n",
    "    mixed_scores = disc(mixed_images, audio_embedding)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=mixed_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "\n",
    "    # Calculate the magnitude of every row\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "\n",
    "    # Penalize the mean squared distance of the gradient norms from 1\n",
    "    penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return penalty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters, model_name: str, epoch=None, cur_step=None):\n",
    "    \"\"\"Plots the gradients flowing through different layers in the network during training.\n",
    "    Can be used for checking for possible gradient vanishing/exploding problems.\n",
    "\n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as\n",
    "    `plot_grad_flow(self.model.named_parameters())` to visualize the gradient flow\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the stats\n",
    "    avg_grads = []\n",
    "    max_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if p.requires_grad and (\"bias\" not in n) and p.grad is not None:\n",
    "            layers.append(n)\n",
    "            avg_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "\n",
    "    # Initialize plot canvas\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(6, 6)\n",
    "\n",
    "    # Plot\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.2, lw=1, color=\"c\")  # Max gradients\n",
    "    plt.bar(np.arange(len(max_grads)), avg_grads, alpha=0.2, lw=1, color=\"b\")  # Mean gradients\n",
    "    plt.hlines(0, 0, len(avg_grads) + 1, lw=2, color=\"k\")  # Zero gradient line\n",
    "    plt.xticks(range(0, len(avg_grads), 1), layers, rotation=\"vertical\")\n",
    "\n",
    "    # Set display options\n",
    "    plt.xlim(left=0, right=len(avg_grads))\n",
    "    plt.ylim(bottom=-0.001, top=0.02)  # Zoom in on the lower gradient regions\n",
    "    plt.xlabel('Layers')\n",
    "    plt.ylabel('average gradient')\n",
    "    plt.title(f'Gradient flow in {model_name}')\n",
    "    plt.grid(True)\n",
    "    plt.legend(\n",
    "        [\n",
    "            Line2D([0], [0], color=\"c\", lw=4),\n",
    "            Line2D([0], [0], color=\"b\", lw=4),\n",
    "            Line2D([0], [0], color=\"k\", lw=4)\n",
    "        ],\n",
    "        [\n",
    "            'max-gradient',\n",
    "            'mean-gradient',\n",
    "            'zero-gradient'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.plot()\n",
    "    if epoch is not None:\n",
    "        print(\"Saving grad flow to png file\")\n",
    "        import covergan_train\n",
    "        plt.savefig(f\"{covergan_train.logger.plots_dir}/grad-flow-{epoch}-{cur_step}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def plot_real_fake_covers(real_cover_tensor: torch.Tensor, fake_cover_tensor: torch.Tensor,\n",
    "                          disc_real_pred: torch.Tensor = None, disc_fake_pred: torch.Tensor = None,\n",
    "                          epoch=None, cur_step=None, plot_saving_dir=\"./plots\"):\n",
    "    sample_count = 5  # max covers to draw\n",
    "\n",
    "    real_cover_tensor = real_cover_tensor[:sample_count]\n",
    "    fake_cover_tensor = fake_cover_tensor[:sample_count]\n",
    "\n",
    "    rows = min(sample_count, len(real_cover_tensor))\n",
    "    cols = 2\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(6 * cols, 6 * rows)\n",
    "    for i in range(rows):\n",
    "        real = real_cover_tensor[i]\n",
    "        fake = fake_cover_tensor[i]\n",
    "\n",
    "        real_pil = to_pil_image(real)\n",
    "        fake_pil = to_pil_image(fake)\n",
    "\n",
    "        real_score = disc_real_pred[i].item() if disc_real_pred is not None else None\n",
    "        fake_score = disc_fake_pred[i].item() if disc_fake_pred is not None else None\n",
    "\n",
    "        for (j, (pil, score)) in enumerate([(real_pil, real_score), (fake_pil, fake_score)]):\n",
    "            plt.subplot(rows, cols, i * cols + j + 1)\n",
    "            plt.imshow(pil)\n",
    "            if score is not None:\n",
    "                plt.text(10, 10, f'{score:.3f}', backgroundcolor='w', fontsize=40.0)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.plot()\n",
    "    if epoch is not None:\n",
    "        print(\"Saving covers to png file\")\n",
    "        import covergan_train\n",
    "        # print(covergan_train.logger.plots_dir)\n",
    "        plt.savefig(f\"{covergan_train.logger.plots_dir}/covers-{epoch}-{cur_step}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyDiffVG uses GPU: False\n",
      "MyGeneratorFixedSixFigs32Good(\n",
      "  (model_): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=223, bias=True)\n",
      "    (1): BatchNorm1d(223, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Linear(in_features=223, out_features=428, bias=True)\n",
      "    (4): BatchNorm1d(428, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Linear(in_features=428, out_features=633, bias=True)\n",
      "    (7): BatchNorm1d(633, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ELU(alpha=1.0)\n",
      "    (9): Linear(in_features=633, out_features=838, bias=True)\n",
      "    (10): BatchNorm1d(838, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ELU(alpha=1.0)\n",
      "    (12): Linear(in_features=838, out_features=1040, bias=True)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 24, kernel_size=(6, 6), stride=(4, 4), padding=(1, 1), bias=False)\n",
      "    (1): LayerNorm((100, 100), eps=1e-05, elementwise_affine=True)\n",
      "    (2): ELU(alpha=1.0)\n",
      "    (3): Conv2d(24, 48, kernel_size=(5, 5), stride=(3, 3), padding=(1, 1), bias=False)\n",
      "    (4): LayerNorm((33, 33), eps=1e-05, elementwise_affine=True)\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Conv2d(48, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): LayerNorm((16, 16), eps=1e-05, elementwise_affine=True)\n",
      "    (8): ELU(alpha=1.0)\n",
      "  )\n",
      "  (adv_layer): Sequential(\n",
      "    (0): Linear(in_features=12297, out_features=192, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=192, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load checkpoint.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bc3374533b74ac6a63b1a25bf141a93"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 9])\n",
      "torch.Size([8, 400, 400, 3])\n",
      "HERE0\n",
      "HERE1\n",
      "tensor(0.0771, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "palette_generator = colorer\n",
    "def generate(z, audio_embedding_disc):\n",
    "    if palette_generator is None:\n",
    "        return gen(z, audio_embedding_disc)\n",
    "    return gen(z, audio_embedding_disc, palette_generator=palette_generator)\n",
    "\n",
    "logger.info(f'PyDiffVG uses GPU: {pydiffvg.get_use_gpu()}')\n",
    "logger.info(gen)\n",
    "logger.info(disc)\n",
    "\n",
    "n_epochs = 1\n",
    "disc_repeats = 1#Discriminator runs per iteration\n",
    "\n",
    "\n",
    "disc_lr = gen_lr = 3e-4\n",
    "\n",
    "z_dim = 9\n",
    "disc_slices = 1\n",
    "checkpoint_root = '/home/dmitriy/OVE/covergan/weights'\n",
    "display_steps = 30\n",
    "backup_epochs = 5\n",
    "bin_steps = 20\n",
    "plot_grad = False\n",
    "c_lambda = 1\n",
    "\n",
    "batch_size=8\n",
    "dataloader = DataLoader(dataset_image,drop_last=True, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=gen_lr, betas=(0.5, 0.9))\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=disc_lr, betas=(0.5, 0.9))\n",
    "cgan_out_name='cgan_out'\n",
    "print(\"Trying to load checkpoint.\")\n",
    "epochs_done = load_checkpoint(checkpoint_root, cgan_out_name, [gen, disc, gen_opt, disc_opt])\n",
    "if epochs_done:\n",
    "    logger.info(f\"Loaded a checkpoint with {epochs_done} epochs done\")\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "shuffle_losses = []\n",
    "val_metrics = []\n",
    "\n",
    "disc_repeat_cnt = 0\n",
    "mean_iteration_disc_loss, mean_shuffle_disc_loss = 0, 0\n",
    "for epoch in range(epochs_done + 1, n_epochs + epochs_done + 1):\n",
    "    for emotion,image in tqdm(dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        emotion = emotion.float().to(device)\n",
    "        print(emotion.shape)\n",
    "        image = image.to(device)\n",
    "        emotion_disc = emotion\n",
    "        cur_batch_size = len(emotion)\n",
    "\n",
    "        def get_fake_pred(should_detach: bool):\n",
    "            # Get noise corresponding to the current batch_size\n",
    "            z = get_noise(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "            if should_detach:\n",
    "                with torch.no_grad():\n",
    "                    fake_covers = generate(z, emotion_disc)\n",
    "                    fig_params = None\n",
    "            else:\n",
    "                # fake_covers, fig_params = gen(z, audio_embedding_disc, emotions, return_figure_params=True)\n",
    "                fake_covers = generate(z, emotion_disc)\n",
    "                fig_params = None\n",
    "            # Make sure that enough shapes were generated\n",
    "            assert len(fake_covers) == len(image)\n",
    "            fake_pred = disc(fake_covers, emotion_disc)\n",
    "\n",
    "            assert len(fake_pred) == len(fake_covers)\n",
    "            return fake_covers, fake_pred, fig_params\n",
    "\n",
    "        # ### Update discriminator with fakes ###\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "\n",
    "        fake_cover_tensor, disc_fake_pred, fig_params = get_fake_pred(should_detach=True)\n",
    "        print(image.shape)\n",
    "        disc_real_pred = disc(image.permute(0,3,1,2), emotion)\n",
    "\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(image)\n",
    "        # Shapes must match\n",
    "        assert tuple(disc_fake_pred.shape) == tuple(disc_real_pred.shape)\n",
    "\n",
    "        # gp = get_gradient_penalty(disc, image.permute(0,3,1,2).data, fake_cover_tensor.data,\n",
    "        #                           emotion)\n",
    "        print('HERE0')\n",
    "        disc_loss = disc_fake_pred.mean() - disc_real_pred.mean()\n",
    "        # Keep track of the average critic loss in this batch\n",
    "        mean_iteration_disc_loss += disc_loss.item() / disc_repeats\n",
    "        disc_repeat_cnt += 1\n",
    "        print('HERE1')\n",
    "        # Update gradients\n",
    "        print(disc_loss)\n",
    "        disc_loss.backward()\n",
    "        if plot_grad:\n",
    "            plot_grad_flow(disc.named_parameters(), \"discriminator (fakes)\", epoch=epoch, cur_step=cur_step)\n",
    "        # Update optimizer\n",
    "        disc_opt.step()\n",
    "\n",
    "        # if USE_SHUFFLING:\n",
    "        #     # ### Update discriminator with matching and shuffled cover-embedding pairs ###\n",
    "        #     disc_opt.zero_grad()\n",
    "        #\n",
    "        #     disc_real_pred = disc(real_cover_tensor, audio_embedding_disc, emotions)\n",
    "        #     disc_shuffle_pred = disc(shuffle_cover_tensor, audio_embedding_disc, emotions)\n",
    "        #     disc_loss = disc_shuffle_pred.mean() - disc_real_pred.mean()\n",
    "        #\n",
    "        #     mean_shuffle_disc_loss += disc_loss.item() / disc_repeats\n",
    "        #\n",
    "        #     # Update gradients\n",
    "        #     disc_loss.backward()\n",
    "        #     if plot_grad:\n",
    "        #         plot_grad_flow(disc.named_parameters(), \"discriminator (shuffled pairs)\", epoch=epoch,\n",
    "        #                        cur_step=cur_step)\n",
    "        #     disc_opt.step()\n",
    "        print('HERE2')\n",
    "        if disc_repeat_cnt == disc_repeats:\n",
    "            # Keep track of the average discriminator loss\n",
    "            discriminator_losses.append(mean_iteration_disc_loss)\n",
    "            shuffle_losses.append(mean_shuffle_disc_loss)\n",
    "            disc_repeat_cnt = 0\n",
    "            mean_iteration_disc_loss, mean_shuffle_disc_loss = 0, 0\n",
    "\n",
    "            # ### Update generator ###\n",
    "            # Zero out the generator gradients\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            # Getting fake shapes, same as in the loop above\n",
    "            fake_cover_tensor, disc_fake_pred, fig_params = get_fake_pred(should_detach=False)\n",
    "\n",
    "            # p_dist = torch.nn.PairwiseDistance()\n",
    "            # dist_loss_sum = torch.tensor(0.0).to(fake_cover_tensor.device)\n",
    "            # for figs in fig_params:\n",
    "            #     cur_batch_dist_loss = torch.tensor(0.0).to(fake_cover_tensor.device)\n",
    "            #     for ind_x, x in enumerate(figs):\n",
    "            #         for ind_y in range(ind_x + 1, len(figs)):\n",
    "            #             a = x[\"center_point\"]\n",
    "            #             b = figs[ind_y][\"center_point\"]\n",
    "            #             cur_batch_dist_loss += 50000 / (p_dist(a, b)) ** 3\n",
    "            #     dist_loss_sum += cur_batch_dist_loss / len(figs)\n",
    "            #\n",
    "            # dist_loss_sum = dist_loss_sum / len(fig_params)\n",
    "            # dist_loss_sum.backward()\n",
    "            # gen_loss = -disc_fake_pred.mean() + dist_loss_sum\n",
    "            gen_loss = -disc_fake_pred.mean()\n",
    "            gen_loss.backward()\n",
    "            if plot_grad:\n",
    "                plot_grad_flow(gen.named_parameters(), \"generator\", epoch=epoch, cur_step=cur_step)\n",
    "\n",
    "            # Update the weights\n",
    "            gen_opt.step()\n",
    "\n",
    "            # Keep track of the generator losses\n",
    "            generator_losses.append(gen_loss.item())\n",
    "\n",
    "        plot_losses(epoch, cur_step, display_steps, bin_steps, [\n",
    "            (\"Generator\", generator_losses),\n",
    "            (\"Discriminator Adversarial\", discriminator_losses),\n",
    "            (\"Discriminator Mismatches\", shuffle_losses)\n",
    "        ])\n",
    "        if cur_step % display_steps == 0:\n",
    "            plot_real_fake_covers(image.permute(0,3,1,2), fake_cover_tensor, disc_real_pred, disc_fake_pred, epoch=epoch,\n",
    "                                  cur_step=cur_step)\n",
    "            save_checkpoint(checkpoint_root, cgan_out_name, epoch, 0, [gen, disc, gen_opt, disc_opt])\n",
    "        cur_step += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a=1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
