{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:18.718841012Z",
     "start_time": "2023-05-11T14:10:17.002543373Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pydiffvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:18.724063772Z",
     "start_time": "2023-05-11T14:10:18.721202540Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import covergan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:18.732367696Z",
     "start_time": "2023-05-11T14:10:18.723909581Z"
    }
   },
   "outputs": [],
   "source": [
    "# labels = {'neutral': 0, 'happy': 1, 'sad': 2, 'surprise': 3, 'fear': 4, 'disgust': 5, 'anger': 6, 'contempt': 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:18.745268093Z",
     "start_time": "2023-05-11T14:10:18.733484823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'covergan' from '/home/dmitriy/OVE/covergan/__init__.py'>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, sys\n",
    "importlib.reload(covergan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:19.501518150Z",
     "start_time": "2023-05-11T14:10:18.745519939Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/home/dmitriy/OVE/official_data/artemis_dataset_release_v0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:19.543488144Z",
     "start_time": "2023-05-11T14:10:19.516689533Z"
    }
   },
   "outputs": [],
   "source": [
    "ndata = data[data['art_style']=='Color_Field_Painting'].groupby(['painting', 'art_style'])['emotion'].apply(lambda x: ','.join(x)).reset_index()\n",
    "ndata = ndata.drop([197,198,199]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:19.545482669Z",
     "start_time": "2023-05-11T14:10:19.543698105Z"
    }
   },
   "outputs": [],
   "source": [
    "# ndata = data[data['art_style']=='Color_Field_Painting'].groupby(['painting', 'art_style'])['emotion'].apply(lambda x: ','.join(x)).reset_index()\n",
    "# ndata = ndata.drop([197,198,199]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T21:05:07.093083693Z",
     "start_time": "2023-05-10T21:05:07.083397210Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import covergan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T21:05:07.117951241Z",
     "start_time": "2023-05-10T21:05:07.093274891Z"
    }
   },
   "outputs": [],
   "source": [
    "# ndata.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T21:03:55.898414612Z",
     "start_time": "2023-05-10T21:03:55.886259764Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "#\n",
    "# # Get the list of all files and directories\n",
    "# path = \"/home/dmitriy/OVE/data\"\n",
    "# dir_list = os.listdir(path)\n",
    "# dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T21:03:55.898587981Z",
     "start_time": "2023-05-10T21:03:55.886374455Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "#\n",
    "# # Get the list of all files and directories\n",
    "# path = \"/home/dmitriy/OVE/data\"\n",
    "# dir_list = os.listdir(path)\n",
    "# dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T22:34:47.537160563Z",
     "start_time": "2023-05-10T22:34:47.530477803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'smiling',\n 1: 'sad',\n 2: 'surprise',\n 3: 'fear',\n 4: 'disgust',\n 5: 'anger',\n 6: 'contempt'}"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label= {'smiling': 0, 'sad': 1, 'surprise': 2, 'fear': 3, 'disgust': 4, 'anger': 5, 'contempt': 6}\n",
    "# key_val = {v:k for k,v in label.items()}\n",
    "# key_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T21:03:55.898698761Z",
     "start_time": "2023-05-10T21:03:55.886459054Z"
    }
   },
   "outputs": [],
   "source": [
    "# inputs = []\n",
    "# labels = []\n",
    "# for d1 in dir_list:\n",
    "#     dir_new = os.path.join(path,d1)\n",
    "#     images = os.listdir(dir_new)\n",
    "#     for image in images:\n",
    "#         inputs.append(image)\n",
    "#         labels.append(label[d1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T21:03:55.898734188Z",
     "start_time": "2023-05-10T21:03:55.886505798Z"
    }
   },
   "outputs": [],
   "source": [
    "# data = pd.DataFrame({\"image\":inputs,\"label\":labels})\n",
    "# data.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T21:03:55.933701493Z",
     "start_time": "2023-05-10T21:03:55.930312205Z"
    }
   },
   "outputs": [],
   "source": [
    "# data[data['image'].isin(data[data.duplicated('image')]['image'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-10T22:30:06.225884060Z",
     "start_time": "2023-05-10T22:30:06.038663027Z"
    }
   },
   "outputs": [],
   "source": [
    "# from colorer.music_palette_dataset import PaletteDatasetSmiles,ImageDatasetSmiles\n",
    "# dataset = PaletteDatasetSmiles(8,156,'name','/home/dmitriy/OVE/covergan/weights','/home/dmitriy/OVE/data1',pandas_dir='/home/dmitriy/OVE/out.csv')\n",
    "# dataset_image = ImageDatasetSmiles(8,156,'name','/home/dmitriy/OVE/covergan/weights','/home/dmitriy/OVE/data1',pandas_dir='/home/dmitriy/OVE/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-10T22:30:06.259158764Z",
     "start_time": "2023-05-10T22:30:06.215430487Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset_image[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.040126773Z",
     "start_time": "2023-05-11T14:10:19.546439643Z"
    }
   },
   "outputs": [],
   "source": [
    "from colorer.music_palette_dataset import PaletteDataset,ImageDataset\n",
    "dataset = PaletteDataset(8,1599,'name','/home/dmitriy/OVE/covergan/weights','/home/dmitriy/OVE/wikiart',pandas_dir='/home/dmitriy/OVE/official_data/artemis_dataset_release_v0.csv')\n",
    "dataset_image = ImageDataset(8,1599,'name','/home/dmitriy/OVE/covergan/weights','/home/dmitriy/OVE/wikiart',pandas_dir='/home/dmitriy/OVE/official_data/artemis_dataset_release_v0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.042751426Z",
     "start_time": "2023-05-11T14:10:21.041200948Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset.data = ndata\n",
    "dataset_image.data = ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1., 1., 0., 0., 0., 0., 1., 0., 0.]),\n tensor([0.1725, 0.1608, 0.1765, 0.1804, 0.1686, 0.1882, 0.1961, 0.1804, 0.2000,\n         0.1490, 0.1333, 0.1451, 0.1176, 0.0941, 0.1059, 0.5098, 0.4941, 0.4784,\n         0.2706, 0.2510, 0.2510, 0.3961, 0.3804, 0.3686]))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.071354445Z",
     "start_time": "2023-05-11T14:10:21.045107128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 'something else',\n 1: 'sadness',\n 2: 'contentment',\n 3: 'awe',\n 4: 'amusement',\n 5: 'excitement',\n 6: 'fear',\n 7: 'disgust',\n 8: 'anger'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_key = dict(zip(data.emotion.unique(), data.index))\n",
    "key_val = {v:k for k,v in val_key.items()}\n",
    "key_val"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.100566150Z",
     "start_time": "2023-05-11T14:10:21.081085059Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.100787523Z",
     "start_time": "2023-05-11T14:10:21.091633415Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(len(dataset_image)):\n",
    "#     print(dataset_image[i][1].shape)\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.103543625Z",
     "start_time": "2023-05-11T14:10:21.095432854Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.112542165Z",
     "start_time": "2023-05-11T14:10:21.103703107Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.126754758Z",
     "start_time": "2023-05-11T14:10:21.113025391Z"
    }
   },
   "outputs": [],
   "source": [
    "from colorer.models.colorer_dropout import Colorer2\n",
    "z_dim=128\n",
    "num_gen_layers=3\n",
    "colors_count = 8\n",
    "colorer = Colorer2(\n",
    "        z_dim=z_dim,\n",
    "        audio_embedding_dim=9,\n",
    "        num_layers=num_gen_layers,\n",
    "        colors_count=colors_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.174861571Z",
     "start_time": "2023-05-11T14:10:21.127520414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Colorer2(\n  (model_): Sequential(\n    (0): Linear(in_features=137, out_features=94, bias=True)\n    (1): Dropout(p=0.2, inplace=False)\n    (2): BatchNorm1d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): LeakyReLU(negative_slope=0.2)\n    (4): Linear(in_features=94, out_features=51, bias=True)\n    (5): Dropout(p=0.2, inplace=False)\n    (6): BatchNorm1d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2)\n    (8): Linear(in_features=51, out_features=24, bias=True)\n    (9): Sigmoid()\n  )\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.174997829Z",
     "start_time": "2023-05-11T14:10:21.174736377Z"
    }
   },
   "outputs": [],
   "source": [
    "from colorer.models.gan_colorer import ColorerDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.175057707Z",
     "start_time": "2023-05-11T14:10:21.174964816Z"
    }
   },
   "outputs": [],
   "source": [
    "disc = ColorerDiscriminator(audio_embedding_dim=9, num_layers=3,colors_count= colors_count).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.175226794Z",
     "start_time": "2023-05-11T14:10:21.175030181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ColorerDiscriminator(\n  (adv_layer): Sequential(\n    (0): Linear(in_features=33, out_features=23, bias=True)\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=23, out_features=13, bias=True)\n    (4): LeakyReLU(negative_slope=0.2)\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=13, out_features=8, bias=True)\n    (7): Sigmoid()\n  )\n)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.175309610Z",
     "start_time": "2023-05-11T14:10:21.175096001Z"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_mse_loss(input, target, weight=None):\n",
    "    if weight is None:\n",
    "        max_weight = input.size()[1]\n",
    "        weight = torch.tensor([(max_weight - i // 3) // 3 for i in range(max_weight)]).to(input.device)\n",
    "        weight = weight.repeat((len(input), 1))\n",
    "    return (weight * (input - target) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.218796503Z",
     "start_time": "2023-05-11T14:10:21.175173085Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_noise(n_samples, input_dim, device):\n",
    "    \"\"\"\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    \"\"\"\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.218992752Z",
     "start_time": "2023-05-11T14:10:21.218609160Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size=64\n",
    "train_dataloader = DataLoader(dataset,drop_last=True, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.219065181Z",
     "start_time": "2023-05-11T14:10:21.218939236Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_checkpoint_filename(checkpoint_root: str, checkpoint_name: str, epoch: int = None) -> str:\n",
    "    suffix = f\"-{epoch}\" if epoch is not None else \"\"\n",
    "    return f\"{checkpoint_root}/{checkpoint_name}{suffix}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.656940661Z",
     "start_time": "2023-05-11T14:10:21.219027613Z"
    }
   },
   "outputs": [],
   "source": [
    "from captioner_train import logger\n",
    "import os\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_root: str, checkpoint_name: str,\n",
    "                    models) -> int:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    filename = get_checkpoint_filename(checkpoint_root, checkpoint_name)\n",
    "\n",
    "    if os.path.isfile(filename):\n",
    "        logger.info(f\"Found {filename}, loading\")\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        for i, model in enumerate(models):\n",
    "            model.load_state_dict(checkpoint[f\"{i}_state_dict\"])\n",
    "            print('Loaded')\n",
    "        epochs_done = checkpoint[f\"epochs_done\"]\n",
    "        logger.info(f\"{filename} loaded\")\n",
    "        return epochs_done\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.659122802Z",
     "start_time": "2023-05-11T14:10:21.657219576Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.662892849Z",
     "start_time": "2023-05-11T14:10:21.661612903Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_root: str, checkpoint_name: str, epochs_done: int, backup_epochs: int,\n",
    "                    models):\n",
    "    checkpoint_dict = {}\n",
    "    for i, model in enumerate(models):\n",
    "        checkpoint_dict[f\"{i}_state_dict\"] = model.state_dict()\n",
    "    checkpoint_dict[f\"epochs_done\"] = epochs_done\n",
    "\n",
    "    if not backup_epochs:\n",
    "        # Unconditional save\n",
    "        filename = get_checkpoint_filename(checkpoint_root, checkpoint_name)\n",
    "        torch.save(checkpoint_dict, filename)\n",
    "        logger.info(f\"{filename} saved\")\n",
    "    if backup_epochs and epochs_done and epochs_done % backup_epochs == 0:\n",
    "        # Regular backup\n",
    "        filename = get_checkpoint_filename(checkpoint_root, checkpoint_name, epochs_done)\n",
    "        torch.save(checkpoint_dict, filename)\n",
    "        logger.info(f\"Backup {filename} saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:21.710982702Z",
     "start_time": "2023-05-11T14:10:21.664211511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ColorerDiscriminator(\n  (adv_layer): Sequential(\n    (0): Linear(in_features=33, out_features=23, bias=True)\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=23, out_features=13, bias=True)\n    (4): LeakyReLU(negative_slope=0.2)\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=13, out_features=8, bias=True)\n    (7): Sigmoid()\n  )\n)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "n_epochs = 400 # training_params[\"n_epochs\"]\n",
    "lr1 = 2e-4\n",
    "lr2 = 2e-4\n",
    "checkpoint_root ='/home/dmitriy/OVE/covergan/weights'\n",
    "backup_epochs = 500\n",
    "gen_opt = torch.optim.Adam(colorer.parameters(), lr=lr1,betas=(0.5, 0.9))\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr2,betas=(0.5, 0.9))\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "# model_name = f'colorer_{colorer.color_type}_{colorer.colors_count}_colors'\n",
    "model_name = f'1f_colorer_{colorer.colors_count}_colors_{train_dataloader.dataset.sorted_color}'\n",
    "print(\"Trying to load checkpoint.\")\n",
    "epochs_done = load_checkpoint(checkpoint_root, model_name, [colorer, disc, gen_opt, disc_opt])\n",
    "\n",
    "if epochs_done:\n",
    "    logger.info(f\"Loaded a checkpoint with {epochs_done} epochs done\")\n",
    "disc_repeats = 60\n",
    "step = 0\n",
    "log_interval = 20\n",
    "mean_iteration_disc_loss= 0\n",
    "cur_step = 0\n",
    "\n",
    "for epoch in tqdm(range(epochs_done + 1, n_epochs + epochs_done + 1)):\n",
    "    colorer.train()\n",
    "    running_D_test_loss = 0.0\n",
    "    running_G_test_loss = 0.0\n",
    "    count = 0\n",
    "    count_disc = 0\n",
    "    for emotion, palette in train_dataloader:\n",
    "        palette = palette.to(device)\n",
    "        cur_batch_size = len(emotion)\n",
    "        emotion = emotion.float().to(device)\n",
    "        z = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        fake_labels = torch.nn.functional.one_hot(torch.randint(0, 9, (cur_batch_size,)), 9)\n",
    "        # true_labels = torch.ones(batch_size).to(device).unsqueeze(-1)\n",
    "        if count%disc_repeats==0:\n",
    "            fake_covers = colorer(z, fake_labels)\n",
    "            disc_opt.zero_grad()\n",
    "            disc_real_pred = disc(palette, emotion)\n",
    "            real_disc_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred).to(device))\n",
    "            fake_disc_pred = disc(fake_covers.detach(), fake_labels)\n",
    "            gen_disc_loss = criterion(fake_disc_pred, torch.zeros_like(fake_disc_pred).to(device))\n",
    "            disc_loss = (real_disc_loss + gen_disc_loss) / 2\n",
    "            disc_loss.backward()\n",
    "            disc_opt.step()\n",
    "            mean_iteration_disc_loss = disc_loss.item()\n",
    "            running_D_test_loss += mean_iteration_disc_loss\n",
    "            count_disc+=1\n",
    "\n",
    "        disc_opt.zero_grad()\n",
    "        fake_covers =  colorer(z, fake_labels)\n",
    "        fake_disc_pred_gen = disc(fake_covers, fake_labels)\n",
    "        gen_loss = criterion(fake_disc_pred_gen, torch.ones_like(fake_disc_pred_gen).to(device))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "        running_G_test_loss += gen_loss.item()\n",
    "\n",
    "        count+=1\n",
    "        cur_step+=1\n",
    "\n",
    "    if (epoch + 1) % log_interval == 0:\n",
    "        save_checkpoint(checkpoint_root, model_name, epoch, 0, [colorer, disc, gen_opt, disc_opt])\n",
    "        avg_G_test_loss = running_G_test_loss / (count + 1)\n",
    "        avg_D_test_loss = running_D_test_loss / (count_disc+ 1)\n",
    "        print('Train LOSS: colorer {}, disc {}'.format(avg_G_test_loss, avg_D_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:53.128503757Z",
     "start_time": "2023-05-11T14:10:53.082995642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colorer.load_state_dict(torch.load('/home/dmitriy/OVE/covergan/weights/1f_colorer_8_colors_sorted.pt')['0_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:54.074810207Z",
     "start_time": "2023-05-11T14:10:54.033581701Z"
    }
   },
   "outputs": [],
   "source": [
    "from outer.models.discriminator import Discriminator\n",
    "from outer.models.my_gen_fixed_6figs32_good import MyGeneratorFixedSixFigs32Good\n",
    "\n",
    "generator_type = MyGeneratorFixedSixFigs32Good\n",
    "discriminator_type = Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:54.317485960Z",
     "start_time": "2023-05-11T14:10:54.311891415Z"
    }
   },
   "outputs": [],
   "source": [
    "num_gen_layers = 4\n",
    "num_disc_conv_layers = 3\n",
    "num_disc_linear_layers = 2\n",
    "z_dim = 128 # Dimension of the noise vector\n",
    "# z_dim = 512  # Dimension of the noise vector\n",
    "\n",
    "# Painter properties\n",
    "path_count = 3\n",
    "path_segment_count = 4\n",
    "max_stroke_width = 0.01  # relative to the canvas size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:54.514195945Z",
     "start_time": "2023-05-11T14:10:54.501568607Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "MyGeneratorFixedSixFigs32Good(\n  (emb): Embedding(9, 9)\n  (model_): Sequential(\n    (0): Linear(in_features=137, out_features=256, bias=True)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n    (3): Linear(in_features=256, out_features=512, bias=True)\n    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n    (6): Linear(in_features=512, out_features=1024, bias=True)\n    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n    (9): Linear(in_features=1024, out_features=1040, bias=True)\n    (10): Tanh()\n  )\n)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = MyGeneratorFixedSixFigs32Good(z_dim,9,num_gen_layers,128,max_stroke_width)\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:58.450420478Z",
     "start_time": "2023-05-11T14:10:58.445137795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Discriminator(\n  (emb): Embedding(9, 9)\n  (model): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): LeakyReLU(negative_slope=0.2)\n    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2)\n    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (7): Conv2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (8): AdaptiveAvgPool2d(output_size=1)\n    (9): Sigmoid()\n  )\n  (adv_layer): Sequential(\n    (0): Linear(in_features=41, out_features=1, bias=True)\n    (1): Sigmoid()\n  )\n)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator(128,9,3,2)\n",
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:58.687381261Z",
     "start_time": "2023-05-11T14:10:58.660318788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.0118, 0.0039, 0.0235],\n         [0.0275, 0.0118, 0.1412],\n         [0.0196, 0.0078, 0.0902],\n         ...,\n         [0.1255, 0.1569, 0.2078],\n         [0.1294, 0.1765, 0.2235],\n         [0.1176, 0.1451, 0.2157]],\n\n        [[0.0000, 0.0078, 0.1098],\n         [0.0549, 0.0980, 0.3373],\n         [0.0863, 0.1608, 0.3882],\n         ...,\n         [0.1529, 0.2275, 0.4549],\n         [0.0706, 0.1333, 0.3333],\n         [0.1451, 0.1765, 0.3176]],\n\n        [[0.0000, 0.0078, 0.1216],\n         [0.1922, 0.2510, 0.5137],\n         [0.1255, 0.2314, 0.5020],\n         ...,\n         [0.1608, 0.2706, 0.5608],\n         [0.1647, 0.2431, 0.4745],\n         [0.0941, 0.1373, 0.2471]],\n\n        ...,\n\n        [[0.0000, 0.0118, 0.0784],\n         [0.0980, 0.1176, 0.3765],\n         [0.1176, 0.1647, 0.3922],\n         ...,\n         [0.1412, 0.2314, 0.4980],\n         [0.1451, 0.2000, 0.4392],\n         [0.0118, 0.0235, 0.0431]],\n\n        [[0.0000, 0.0000, 0.1176],\n         [0.1294, 0.1490, 0.4157],\n         [0.1216, 0.1490, 0.4549],\n         ...,\n         [0.1333, 0.2275, 0.5176],\n         [0.1451, 0.2078, 0.4118],\n         [0.0039, 0.0275, 0.0902]],\n\n        [[0.0000, 0.0039, 0.0941],\n         [0.1020, 0.1255, 0.3216],\n         [0.1137, 0.1412, 0.4078],\n         ...,\n         [0.0000, 0.0275, 0.1922],\n         [0.0000, 0.0157, 0.1412],\n         [0.0118, 0.0000, 0.0824]]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "dataset_image[6][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:58.884699221Z",
     "start_time": "2023-05-11T14:10:58.877574521Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_losses(epoch: int, cur_step: int, display_steps: int, bin_steps: int, losses: [(str, [float])]):\n",
    "    if cur_step % display_steps == 0 and cur_step > 0:\n",
    "        loss_stats = []\n",
    "\n",
    "        for loss_name, loss_values in losses:\n",
    "            loss_mean = sum(loss_values[-display_steps:]) / display_steps\n",
    "            if \"loss\" not in loss_name.lower() and \"metric\" not in loss_name.lower():\n",
    "                loss_name += \" Loss\"\n",
    "            loss_stats.append(f\"{loss_name}: {loss_mean}\")\n",
    "\n",
    "            num_examples = (len(loss_values) // bin_steps) * bin_steps\n",
    "            plt.plot(\n",
    "                range(num_examples // bin_steps),\n",
    "                torch.Tensor(loss_values[:num_examples]).view(-1, bin_steps).mean(1),\n",
    "                label=loss_name\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Epoch {epoch} (step {cur_step}): \" + \", \".join(loss_stats))\n",
    "        plt.legend()\n",
    "        print(\"Saving losses to png file\")\n",
    "        import covergan_train\n",
    "        # plt.savefig(f\"{covergan_train.logger.plots_dir}/losses-{epoch}-{cur_step}.png\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    elif cur_step == 0:\n",
    "        logger.info(\"The training is working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:59.094512709Z",
     "start_time": "2023-05-11T14:10:59.088506729Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:59.356466942Z",
     "start_time": "2023-05-11T14:10:59.351952210Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_gradient_penalty(disc, real, fake, audio_embedding):\n",
    "    # Mix the images together\n",
    "    epsilon = torch.rand((real.size(0), 1, 1, 1), device=real.device)\n",
    "    mixed_images = (real * epsilon + fake * (1 - epsilon)).requires_grad_(True)\n",
    "\n",
    "    # Calculate the critic's scores on the mixed images\n",
    "    mixed_scores = disc(mixed_images, audio_embedding)\n",
    "\n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=mixed_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "\n",
    "    # Calculate the magnitude of every row\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "\n",
    "    # Penalize the mean squared distance of the gradient norms from 1\n",
    "    penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:10:59.593537088Z",
     "start_time": "2023-05-11T14:10:59.581707092Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_grad_flow(named_parameters, model_name: str, epoch=None, cur_step=None):\n",
    "    \"\"\"Plots the gradients flowing through different layers in the network during training.\n",
    "    Can be used for checking for possible gradient vanishing/exploding problems.\n",
    "\n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as\n",
    "    `plot_grad_flow(self.model.named_parameters())` to visualize the gradient flow\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the stats\n",
    "    avg_grads = []\n",
    "    max_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if p.requires_grad and (\"bias\" not in n) and p.grad is not None:\n",
    "            layers.append(n)\n",
    "            avg_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "\n",
    "    # Initialize plot canvas\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(6, 6)\n",
    "\n",
    "    # Plot\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.2, lw=1, color=\"c\")  # Max gradients\n",
    "    plt.bar(np.arange(len(max_grads)), avg_grads, alpha=0.2, lw=1, color=\"b\")  # Mean gradients\n",
    "    plt.hlines(0, 0, len(avg_grads) + 1, lw=2, color=\"k\")  # Zero gradient line\n",
    "    plt.xticks(range(0, len(avg_grads), 1), layers, rotation=\"vertical\")\n",
    "\n",
    "    # Set display options\n",
    "    plt.xlim(left=0, right=len(avg_grads))\n",
    "    plt.ylim(bottom=-0.001, top=0.02)  # Zoom in on the lower gradient regions\n",
    "    plt.xlabel('Layers')\n",
    "    plt.ylabel('average gradient')\n",
    "    plt.title(f'Gradient flow in {model_name}')\n",
    "    plt.grid(True)\n",
    "    plt.legend(\n",
    "        [\n",
    "            Line2D([0], [0], color=\"c\", lw=4),\n",
    "            Line2D([0], [0], color=\"b\", lw=4),\n",
    "            Line2D([0], [0], color=\"k\", lw=4)\n",
    "        ],\n",
    "        [\n",
    "            'max-gradient',\n",
    "            'mean-gradient',\n",
    "            'zero-gradient'\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.plot()\n",
    "    if epoch is not None:\n",
    "        print(\"Saving grad flow to png file\")\n",
    "        import covergan_train\n",
    "        # plt.savefig(f\"{covergan_train.logger.plots_dir}/grad-flow-{epoch}-{cur_step}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:11:00.026636256Z",
     "start_time": "2023-05-11T14:11:00.023444467Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_real_fake_covers(emotions,real_cover_tensor: torch.Tensor, fake_cover_tensor: torch.Tensor,\n",
    "                          disc_real_pred: torch.Tensor = None, disc_fake_pred: torch.Tensor = None,\n",
    "                          epoch=None, cur_step=None, plot_saving_dir=\"./plots\"):\n",
    "    sample_count = 5  # max covers to draw\n",
    "\n",
    "    real_cover_tensor = real_cover_tensor[:sample_count]\n",
    "    fake_cover_tensor = fake_cover_tensor[:sample_count]\n",
    "    emotions = emotions[:sample_count].numpy()\n",
    "\n",
    "    rows = min(sample_count, len(real_cover_tensor))\n",
    "    cols = 2\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(6 * cols, 6 * rows)\n",
    "    for i in range(rows):\n",
    "        real = real_cover_tensor[i]\n",
    "        fake = fake_cover_tensor[i]\n",
    "        emotion = emotions[i]\n",
    "        emo = []\n",
    "        for j in range(len(emotion)):\n",
    "            if emotion[j]==1:\n",
    "                emo.append(key_val[j])\n",
    "        # real_pil = to_pil_image(real,mode=\"RGB\")\n",
    "        fake_pil = to_pil_image(fake,mode=\"RGB\")\n",
    "\n",
    "        real_score = disc_real_pred[i].item() if disc_real_pred is not None else None\n",
    "        fake_score = disc_fake_pred[i].item() if disc_fake_pred is not None else None\n",
    "\n",
    "        for (j, (pil, score)) in enumerate([(real.permute(1,2,0), real_score), (fake_pil, fake_score)]):\n",
    "            plt.subplot(rows, cols, i * cols + j + 1)\n",
    "            plt.imshow(pil)\n",
    "            if score is not None:\n",
    "                plt.text(10, 10, f'{emo}', backgroundcolor='w', fontsize=10.0)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.plot()\n",
    "    if epoch is not None:\n",
    "        print(\"Saving covers to png file\")\n",
    "        import covergan_train\n",
    "        # print(covergan_train.logger.plots_dir)\n",
    "        # plt.savefig(f\"{covergan_train.logger.plots_dir}/covers-{epoch}-{cur_step}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:11:00.521724179Z",
     "start_time": "2023-05-11T14:11:00.515924659Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset_image,drop_last=True, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:11:00.886546759Z",
     "start_time": "2023-05-11T14:11:00.880610747Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def mismatching_permute(t: torch.Tensor) -> torch.Tensor:\n",
    "    shift = random.randrange(start=0, stop=len(t))\n",
    "    return torch.cat((t[shift:], t[:shift]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-11T12:54:02.305828643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyDiffVG uses GPU: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/199 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "palette_generator = colorer\n",
    "def generate(z, audio_embedding_disc):\n",
    "    if palette_generator is None:\n",
    "        return gen(z, audio_embedding_disc)\n",
    "    return gen(z, audio_embedding_disc, palette_generator=palette_generator)\n",
    "\n",
    "logger.info(f'PyDiffVG uses GPU: {pydiffvg.get_use_gpu()}')\n",
    "# logger.info(gen)\n",
    "# logger.info(disc)\n",
    "\n",
    "n_epochs = 1000\n",
    "disc_repeats = 1#Discriminator runs per iteration\n",
    "\n",
    "\n",
    "disc_lr = gen_lr = 0.0002\n",
    "\n",
    "disc_slices = 1\n",
    "checkpoint_root = '/home/dmitriy/OVE/covergan/weights'\n",
    "display_steps = 5\n",
    "backup_epochs = 5\n",
    "bin_steps = 20\n",
    "plot_grad = False\n",
    "c_lambda = 10\n",
    "new_loss = torch.nn.BCELoss()\n",
    "new_loss_log = torch.nn.BCEWithLogitsLoss()\n",
    "batch_size=8\n",
    "dataloader = DataLoader(dataset_image,drop_last=True, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=gen_lr)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=disc_lr)\n",
    "cgan_out_name='cgan_out'\n",
    "print(\"Trying to load checkpoint.\")\n",
    "epochs_done = load_checkpoint(checkpoint_root, cgan_out_name, [gen, disc, gen_opt, disc_opt])\n",
    "if epochs_done:\n",
    "    logger.info(f\"Loaded a checkpoint with {epochs_done} epochs done\")\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "shuffle_losses = []\n",
    "val_metrics = []\n",
    "\n",
    "disc_repeat_cnt = 0\n",
    "mean_iteration_disc_loss, mean_shuffle_disc_loss = 0, 0\n",
    "for epoch in range(epochs_done + 1, n_epochs + epochs_done + 1):\n",
    "    for emotion, image in tqdm(dataloader):\n",
    "        torch.cuda.empty_cache()\n",
    "        emotion = emotion.float().to(device)\n",
    "        image = image.permute(0, 3, 1, 2).to(device)\n",
    "        cur_batch_size = len(emotion)\n",
    "\n",
    "        z = get_noise(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "        fake_labels = torch.nn.functional.one_hot(torch.randint(0, 9, (cur_batch_size,)), 9).to(device)\n",
    "\n",
    "        true_labels = torch.ones(batch_size).to(device).unsqueeze(-1)\n",
    "\n",
    "        if cur_step%disc_repeats==0:\n",
    "          fake_covers = generate(z, fake_labels)\n",
    "          disc_opt.zero_grad()\n",
    "          disc_real_pred = disc(image, emotion)\n",
    "\n",
    "\n",
    "          real_disc_loss = new_loss(disc_real_pred, true_labels)\n",
    "          fake_disc_pred = disc(fake_covers.detach(), fake_labels)\n",
    "\n",
    "          gen_disc_loss = new_loss(fake_disc_pred, torch.zeros(batch_size).unsqueeze(-1).to(device))\n",
    "\n",
    "          disc_loss = (real_disc_loss + gen_disc_loss) / 2\n",
    "          # disc_loss.backward()\n",
    "          # print(\"wow\")\n",
    "          disc_opt.step()\n",
    "          mean_iteration_disc_loss = disc_loss.item()\n",
    "          if cur_step % display_steps == 0:\n",
    "            plot_losses(epoch, cur_step, display_steps, bin_steps, [\n",
    "                (\"Generator\", generator_losses),\n",
    "                (\"Discriminator Adversarial\", discriminator_losses),\n",
    "                (\"Discriminator Mismatches\", shuffle_losses)\n",
    "            ])\n",
    "            print(image,image.shape)\n",
    "            print(\"?\")\n",
    "            print(fake_covers, fake_covers.shape)\n",
    "            plot_real_fake_covers(fake_labels, image, fake_covers, disc_real_pred, fake_disc_pred, epoch=epoch,\n",
    "                                    cur_step=cur_step)\n",
    "            # save_checkpoint(checkpoint_root, cgan_out_name, epoch, 0, [gen, disc, gen_opt, disc_opt])\n",
    "\n",
    "        discriminator_losses.append(mean_iteration_disc_loss)\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        fake_covers = generate(z, fake_labels)\n",
    "\n",
    "        fake_disc_pred_gen = disc(fake_covers, fake_labels)\n",
    "        gen_loss = new_loss(fake_disc_pred_gen, true_labels)\n",
    "\n",
    "            # gen_loss.backward()\n",
    "            # print(\"wow2\")\n",
    "        gen_opt.step()\n",
    "\n",
    "        generator_losses.append(gen_loss.item())\n",
    "\n",
    "\n",
    "        \n",
    "        cur_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:11:07.586190859Z",
     "start_time": "2023-05-11T14:11:07.579084667Z"
    }
   },
   "outputs": [],
   "source": [
    "z = get_noise(2, z_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:15:10.457188863Z",
     "start_time": "2023-05-11T14:15:10.451725601Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader2 = DataLoader(dataset_image,drop_last=True, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:15:10.860846395Z",
     "start_time": "2023-05-11T14:15:10.854327620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 4.2698e-01, -5.2490e-01,  1.2446e+00,  3.5065e-02, -1.6504e+00,\n         -1.8162e+00, -2.8920e-01, -7.5722e-01, -8.4698e-01,  8.0860e-01,\n          4.6227e-02,  5.9134e-01, -2.0603e+00, -6.9880e-01,  4.3531e-01,\n          4.3921e-01, -1.4697e-01,  4.9175e-01, -5.1314e-01,  1.0648e+00,\n         -2.2883e+00,  7.6505e-01, -4.6528e-01,  1.0786e+00,  1.0337e+00,\n          1.4420e+00, -9.6786e-02, -7.5919e-01, -1.9036e-01,  3.9844e-02,\n          1.3244e+00, -2.4549e+00, -2.7862e-01, -7.7341e-01,  2.8570e+00,\n         -1.7264e+00, -7.2327e-01,  1.7566e+00,  6.6072e-01,  1.0166e+00,\n         -8.3124e-02, -2.3808e-02, -5.9228e-01, -1.2927e+00,  1.8454e-01,\n          1.1257e-01,  2.0442e-01,  2.2467e-01,  7.7725e-01,  5.2655e-01,\n          8.7759e-01, -8.4255e-01, -8.3461e-02, -1.9706e+00,  1.6183e+00,\n          2.4833e-04, -1.5297e+00,  1.4483e+00,  8.4589e-01, -5.5007e-01,\n         -5.0878e-01, -9.1698e-01, -3.3156e-01,  9.0467e-01,  1.1523e+00,\n         -4.0419e-01,  3.1436e-01,  8.1315e-01,  9.8772e-01,  2.5743e-01,\n         -1.4998e+00,  1.7797e-01, -1.6187e-01,  1.1381e+00,  3.7657e-01,\n         -1.3081e-01, -2.8040e-02, -2.6491e-01, -1.8865e+00,  4.0722e-01,\n          7.2702e-01, -3.1223e-01, -3.2923e-01,  1.7940e+00,  1.4852e+00,\n         -1.2347e+00,  1.8904e-01,  1.5829e+00, -2.9110e-02,  1.3114e+00,\n          1.0321e-01, -1.3121e+00,  7.2493e-01, -3.2568e-01, -7.5476e-01,\n          7.1538e-01,  1.7415e-01, -9.8839e-01,  4.7411e-01,  1.6122e+00,\n         -1.0072e+00, -1.6507e-01,  1.3387e+00, -9.3385e-01,  6.7823e-01,\n         -1.3518e+00, -7.5414e-02, -1.4975e+00, -6.4714e-01,  1.3129e+00,\n          1.7535e+00, -1.4994e-02,  7.9770e-01,  1.3132e+00, -1.7436e+00,\n         -3.2366e-01, -1.5103e-01,  4.9246e-02,  3.0286e-02, -1.6056e+00,\n          1.4067e+00,  8.7411e-01, -4.7133e-01, -4.6769e-02, -4.3282e-01,\n          5.9954e-01, -6.4609e-01,  1.0325e+00],\n        [-6.7596e-01,  1.8480e+00, -3.1434e-01, -7.6484e-01,  9.2585e-01,\n          7.3765e-01, -2.0556e+00,  4.8148e-01, -6.3891e-01, -5.1556e-01,\n         -1.9847e+00,  3.1919e-01,  1.5420e+00,  6.0996e-01,  5.2431e-01,\n         -2.7526e-01, -2.8548e-01,  4.0286e-01,  4.0876e-02,  1.0612e+00,\n          2.9188e-01, -1.8482e-01, -3.2079e-01, -8.8990e-02, -5.4179e-01,\n          4.7080e-01, -1.9341e-01,  1.3687e+00, -1.3978e+00,  3.2818e-01,\n         -1.2814e+00, -2.2704e-01,  2.1741e-01,  8.6230e-02, -8.6208e-01,\n         -1.7590e+00,  5.6472e-01, -1.2415e-01,  1.0494e+00,  1.3986e+00,\n          8.4117e-01,  5.9932e-01,  8.0230e-02,  2.5486e+00, -1.6985e+00,\n          7.3884e-01,  5.3353e-01, -1.6355e+00,  8.1496e-01, -7.2092e-01,\n         -3.0424e-01, -1.8023e-01, -6.7987e-01,  1.2362e-02, -8.4815e-01,\n         -1.6124e+00, -2.0629e-01,  2.0027e-01, -4.0275e-02,  9.9452e-01,\n          1.1354e+00,  5.8129e-01, -1.8482e+00,  2.2168e-01,  2.2681e-01,\n         -2.5129e-02, -1.4984e+00,  2.1720e+00,  2.1899e-02, -1.1127e+00,\n          6.9470e-01, -9.2096e-01,  1.5473e+00, -1.8290e-01, -1.4218e-01,\n         -1.7953e-01, -5.6616e-01,  1.1210e+00, -1.6585e-01, -1.0859e+00,\n         -1.0702e-01,  4.9569e-01, -1.3181e+00,  2.3324e-01,  1.3019e+00,\n         -6.9854e-01, -1.2055e-01, -4.8538e-01,  7.8757e-01,  1.4314e+00,\n         -5.5559e-01, -8.6682e-01, -1.7815e+00,  7.3321e-01, -5.6060e-01,\n          1.6290e+00,  6.4132e-01, -6.5710e-01,  6.7328e-01, -4.9905e-01,\n          4.6015e-01, -6.2613e-01,  3.4903e-01,  6.8370e-02, -1.2522e+00,\n         -1.5941e+00,  7.7223e-01,  6.1461e-01,  3.9399e-02,  1.6738e+00,\n          6.4074e-01, -1.9436e-01,  6.5999e-02,  5.2086e-01, -8.1577e-01,\n         -2.6574e-01,  1.2756e-01, -1.6731e+00, -8.9903e-01, -8.9515e-01,\n          5.8032e-01,  4.2486e-01,  2.8548e-01,  6.7230e-01, -1.6201e-01,\n         -1.9977e+00,  3.0625e-01,  6.8701e-01]])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:15:11.290369189Z",
     "start_time": "2023-05-11T14:15:11.260100232Z"
    }
   },
   "outputs": [],
   "source": [
    "a = next(iter(dataloader2))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:15:12.846002122Z",
     "start_time": "2023-05-11T14:15:12.840879879Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "MyGeneratorFixedSixFigs32Good(\n  (emb): Embedding(9, 9)\n  (model_): Sequential(\n    (0): Linear(in_features=137, out_features=256, bias=True)\n    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n    (3): Linear(in_features=256, out_features=512, bias=True)\n    (4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n    (6): Linear(in_features=512, out_features=1024, bias=True)\n    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n    (9): Linear(in_features=1024, out_features=1040, bias=True)\n    (10): Tanh()\n  )\n)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.load_state_dict(torch.load('/home/dmitriy/OVE/covergan/weights/cgan_out_smile.pt',map_location=torch.device('cpu'))['0_state_dict'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-11T14:15:13.298313622Z",
     "start_time": "2023-05-11T14:15:13.278847281Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-11T14:15:28.543309236Z",
     "start_time": "2023-05-11T14:15:28.457441762Z"
    }
   },
   "outputs": [],
   "source": [
    "z = get_noise(2, z_dim, device=device)\n",
    "b = gen(z, a, palette_generator=colorer,return_diffvg_svg_params=True)[0]\n",
    "for ind, params in enumerate(b):\n",
    "    pydiffvg.save_svg(f'/home/dmitriy/OVE/plots/back_diffvg_svg3_{ind}.svg', *params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from service_utils import add_filter\n",
    "# from service_utils import OverlayFilter\n",
    "# num_samples = 2\n",
    "# # filtered_samples = round(num_samples // 2)\n",
    "# filtered_samples = num_samples\n",
    "# filters = list(OverlayFilter)\n",
    "# for psvg_cover in b[-filtered_samples:]:\n",
    "#     overlay_filter = random.choice(filters)\n",
    "#     add_filter(psvg_cover, overlay_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
